---
layout: default
title: ðŸ’¥ Classical statistics pitfalls and remedies ðŸ’Š
description: ðŸ’¥ ðŸ’Š
---

**Instructor:** [Jean-Baptiste Poline](https://www.mcgill.ca/neuro/jean-baptiste-poline-phd)

## Outline

Most of published results still rely on some statistical inference. With this
lecture, you will

-   get a reminder of the classical statistical framework and learn about the
    issues brought by the use of statistical inference

-   learn (or be reminded of) the notion of effect size, power, positive
    predictive values and the consequences of low powered studies

-   understand the file drawer effect, p-hacking, and know about some solutions.

## Questions you will be able to answer after taking this module

<!-- TODO -->

## Material

-   [code](https://github.com/neurodatascience/QLS-course-materials/tree/main/Lectures/2024/07_statistics)
-   [slides](https://github.com/neurodatascience/QLS-course-materials/tree/main/Lectures/2024/07_statistics/lecture)

## Resources

-   [Think Stats](https://greenteapress.com/thinkstats2/thinkstats2.pdf)

## Pre-recorded lecture video

<div style="display: flex; justify-content: center; margin: 10px">

  <iframe
    width="560"
    height="315"
    src="https://www.youtube.com/embed/lRLtWjkBOzQ?si=hUcuvxEDSqylwOmSO"
    title="YouTube video player"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen>
  </iframe>

</div>

## Extras

{% include xkcd.html xkcd_nb="552" img="correlation"%}

---

<a href="{{ site.url }}/lectures-materials/latest.html"><button>BACK</button></a>
